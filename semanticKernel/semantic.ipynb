{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Semantic Kernel Demo Notebook\n",
    "\n",
    "Welcome to the **Semantic Kernel Demo Notebook**! üéâ\n",
    "\n",
    "In this notebook, we'll explore the powerful capabilities of the Semantic Kernel, a cutting-edge tool designed to enhance your data processing and analysis workflows. Whether you're a data scientist, developer, or just curious about semantic technologies, this demo will provide you with a hands-on experience of how the Semantic Kernel can transform your projects.\n",
    "\n",
    "## What You'll Learn üìö\n",
    "\n",
    "- **Introduction to Semantic Kernel**: Understand the core concepts and architecture.\n",
    "- **Data Processing**: See how the Semantic Kernel can streamline your data workflows.\n",
    "- **Real-World Applications**: Discover practical use cases and examples.\n",
    "\n",
    "## Let's Get Started! üèÅ\n",
    "\n",
    "Ready to dive in? Follow along with the cells below and unleash the power of the Semantic Kernel in your projects. Happy coding! üíª"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install semantic-kernel==1.17.1\n",
    "%pip install python-dotenv==1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the necessary environments variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "#  If we change the .env file you want to override the values loaded in memory\n",
    "load_dotenv(override=True)\n",
    "\n",
    "chat_deployment_name = os.getenv(\"CHAT_DEPLOYMENT_NAME\")\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "base_url = os.getenv(\"OPENAI_BASE_URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use the Semantic Kernel following those steps: ü§ñ‚ú®\n",
    "\n",
    "1. **Select the Best AI Service üèÜ**\n",
    "   - Choose the most suitable AI service to run your prompt.\n",
    "\n",
    "2. **Build the Prompt üõ†Ô∏è**\n",
    "   - Construct the prompt using the provided prompt template.\n",
    "\n",
    "3. **Send the Prompt üöÄ**\n",
    "   - Dispatch the prompt to the selected AI service.\n",
    "\n",
    "4. **Receive and Parse the Response üì¨**\n",
    "   - Collect and interpret the response from the AI service.\n",
    "\n",
    "5. **Return the Response üîÑ**\n",
    "   - Deliver the response from the LLM back to your application.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.utils.logging import setup_logging\n",
    "from semantic_kernel.functions import kernel_function\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
    "from semantic_kernel.connectors.ai.chat_completion_client_base import ChatCompletionClientBase\n",
    "from semantic_kernel.contents.chat_history import ChatHistory\n",
    "from semantic_kernel.functions.kernel_arguments import KernelArguments\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai.prompt_execution_settings.azure_chat_prompt_execution_settings import (\n",
    "    AzureChatPromptExecutionSettings,\n",
    ")\n",
    "\n",
    "# Creating the Kernel object\n",
    "\n",
    "kernel =Kernel()\n",
    "\n",
    "chat_completion = AzureChatCompletion(\n",
    "        service_id=\"azure_openai_chat_completion\",\n",
    "        deployment_name=chat_deployment_name,\n",
    "        api_key=api_key,\n",
    "        endpoint=base_url\n",
    ")\n",
    "\n",
    "# Adding the chat completion service to the kernel\n",
    "kernel.add_service(chat_completion)\n",
    "\n",
    "# Set the logging level of the kernel to DEBUG\n",
    "setup_logging()\n",
    "logging.basicConfig(\n",
    "    format=\"[%(asctime)s - %(name)s:%(lineno)d - %(levelname)s] %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "logging.getLogger(\"kernel\").setLevel(logging.DEBUG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.services.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.get_service(\"azure_openai_chat_completion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai_model_id='gpt-4o' service_id='azure_openai_chat_completion' client=<openai.lib.azure.AsyncAzureOpenAI object at 0x7fd6746c1a00> ai_model_type=<OpenAIModelTypes.CHAT: 'chat'> prompt_tokens=0 completion_tokens=0 total_tokens=0\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatPromptExecutionSettings\n",
    "\n",
    "execution_settings = OpenAIChatPromptExecutionSettings(\n",
    "    max_tokens=1000,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "chat_completion_service = kernel.get_service(service_id=\"azure_openai_chat_completion\")\n",
    "\n",
    "print(chat_completion_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = ChatHistory()\n",
    "\n",
    "chat_history.add_user_message(\"What is the best dishes in all Quebec ?\")\n",
    "\n",
    "response = await chat_completion.get_chat_message_content(\n",
    "    chat_history=chat_history,\n",
    "    settings=execution_settings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.add_assistant_message(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all chat history to a file\n",
    "\n",
    "chat_history.store_chat_history_to_file(\"chat_history.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.add_user_message(\"Give me the recipe how to do the first dish you mentioned ?\")\n",
    "\n",
    "response = await chat_completion.get_chat_message_content(\n",
    "    chat_history=chat_history,\n",
    "    settings=execution_settings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.contents.utils.author_role import AuthorRole\n",
    "from semantic_kernel.contents import ChatMessageContent\n",
    "\n",
    "chat_history = ChatHistory()\n",
    "\n",
    "chat_history.add_message(\n",
    "    ChatMessageContent(\n",
    "        role=AuthorRole.SYSTEM,\n",
    "        content=\"\"\"You are an AI assistant, you provide food recipes only from Saguenay Lac-Saint-Jean.  \n",
    "                 If someone asks you for a recipe from another region, you will say that you don't know it and give \n",
    "                 an alternative from Saguenay Lac-St-Jean.\"\"\",\n",
    "    )\n",
    ")\n",
    "\n",
    "chat_history.add_message(\n",
    "    ChatMessageContent(\n",
    "        role=AuthorRole.USER,\n",
    "        content=\"What is the 10 best dishes you have ?\",\n",
    "    )\n",
    ")\n",
    "\n",
    "response = await chat_completion.get_chat_message_content(\n",
    "    chat_history=chat_history,\n",
    "    settings=execution_settings,\n",
    ")\n",
    "\n",
    "chat_history.add_assistant_message(response.content)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.add_user_message(\"Give me the best recipe from Texas USA ?\")\n",
    "\n",
    "response = await chat_completion.get_chat_message_content(\n",
    "    chat_history=chat_history,\n",
    "    settings=execution_settings,\n",
    ")\n",
    "\n",
    "chat_history.add_assistant_message(response.content)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can provide image to get some information about it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.contents import ChatHistory, ChatMessageContent, ImageContent, TextContent\n",
    "\n",
    "chat_history = ChatHistory()\n",
    "\n",
    "with open(\"link.txt\", \"r\") as file:\n",
    "    file_path = file.readline().strip()\n",
    "\n",
    "chat_history.add_message(\n",
    "    ChatMessageContent(\n",
    "        role=\"user\",\n",
    "        items=[\n",
    "            TextContent(text=\"What‚Äôs in this image?\"),\n",
    "            ImageContent(uri=file_path),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "response = await chat_completion.get_chat_message_content(\n",
    "    chat_history=chat_history,\n",
    "    settings=execution_settings\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üöÄ Chat Completion with Function Calling\n",
    "\n",
    "The most powerful feature of chat completion is the ability to call functions from the model. This enables you to:\n",
    "\n",
    "- ü§ñ Create chat bots that interact with your existing code\n",
    "- üîÑ Automate business processes\n",
    "- üíª Generate code snippets\n",
    "\n",
    "## üåü Simplified with Semantic Kernel\n",
    "\n",
    "Semantic Kernel simplifies this process by:\n",
    "\n",
    "- üìú Automatically describing your functions and their parameters to the model\n",
    "- üîÑ Handling the communication between the model and your code\n",
    "\n",
    "## üß† Understanding the Process\n",
    "\n",
    "To optimize your code and fully leverage this feature, it's important to understand the behind-the-scenes workings of function calling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the service needed for the plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inline function\n",
    "from semantic_kernel.prompt_template import InputVariable, PromptTemplateConfig\n",
    "\n",
    "prompt = \"\"\"\n",
    "   Tell me a joke based on this characters {{$input}} from\n",
    "   the nintendo universe.  If this characters is not from the nintendo universe,\n",
    "   just said that you don't know joke outside the Nintendo universe.\n",
    "\"\"\"\n",
    "\n",
    "execution_settings = AzureChatPromptExecutionSettings(\n",
    "    service_id=\"azure_openai_chat_completion\",\n",
    "    ai_model_id=\"gpt-35-turbo\",\n",
    "    max_tokens=2000,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "prompt_template_config = PromptTemplateConfig(\n",
    "    template=prompt,\n",
    "    name=\"nintendoJoke\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[\n",
    "        InputVariable(name=\"input\", description=\"The user input\", is_required=True),\n",
    "    ],\n",
    "    execution_settings=execution_settings,\n",
    ")\n",
    "\n",
    "joke = kernel.add_function(\n",
    "    function_name=\"nitendoJokeFunc\",\n",
    "    plugin_name=\"jokePlugin\",\n",
    "    prompt_template_config=prompt_template_config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"Mario and Luigi\"\n",
    "\n",
    "funnyJoke = await kernel.invoke(joke, input=input)\n",
    "\n",
    "print(funnyJoke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"Sonic the hedgehog\"\n",
    "\n",
    "funnyJoke = await kernel.invoke(joke, input=input)\n",
    "\n",
    "print(funnyJoke)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a plugin for the Semantic Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.functions import kernel_function\n",
    "\n",
    "class BookMeetingRoomPlugin:\n",
    "    def __init__(self):\n",
    "        self.conf_rooms = [{\n",
    "            \"name\": \"Room Azure View\",\n",
    "            \"capacity\": 10,\n",
    "            \"available\": True\n",
    "        }, {\n",
    "            \"name\": \"Room Office View\",\n",
    "            \"capacity\": 20,\n",
    "            \"available\": True\n",
    "        }, {\n",
    "            \"name\": \"Room Github View\",\n",
    "            \"capacity\": 30,\n",
    "            \"available\": True\n",
    "        }]\n",
    "    \n",
    "    @kernel_function(description=\"Get list of booking rooms available\")\n",
    "    def get_list_of_rooms_available(self):\n",
    "        return [room[\"name\"] for room in self.conf_rooms if room[\"available\"]]\n",
    "\n",
    "    @kernel_function(description=\"Book a meeting room and return the name of the available room\")\n",
    "    def book_room(self,number_of_people: int):\n",
    "        for room in self.conf_rooms:\n",
    "            if room[\"available\"] and room[\"capacity\"] >= number_of_people:\n",
    "                room[\"available\"] = False\n",
    "                return f\"Room {room['name']} has been booked for {number_of_people} people.\"\n",
    "        return \"No available rooms found.\"\n",
    "    \n",
    "    @kernel_function(description=\"Cancel a meeting room and return the cancellation status\")\n",
    "    def cancel_room(self, room_name: str):\n",
    "        for room in self.conf_rooms:\n",
    "            if room[\"name\"] == room_name:\n",
    "                room[\"available\"] = True\n",
    "                return f\"Room {room['name']} has been cancelled.\"\n",
    "        return \"Room not found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelPlugin(name='meetingRoomPlugin', description=None, functions={'book_room': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='book_room', plugin_name='meetingRoomPlugin', description='Book a meeting room and return the name of the available room', parameters=[KernelParameterMetadata(name='number_of_people', description=None, default_value=None, type_='int', is_required=True, type_object=<class 'int'>, schema_data={'type': 'integer'}, include_in_function_choices=True)], is_prompt=False, is_asynchronous=False, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='Any', is_required=True, type_object=None, schema_data={'type': 'object'}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x7fd6725b53d0>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x7fd6725b5910>, method=<bound method BookMeetingRoomPlugin.book_room of <__main__.BookMeetingRoomPlugin object at 0x7fd673b90050>>, stream_method=None), 'cancel_room': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='cancel_room', plugin_name='meetingRoomPlugin', description='Cancel a meeting room and return the cancellation status', parameters=[KernelParameterMetadata(name='room_name', description=None, default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string'}, include_in_function_choices=True)], is_prompt=False, is_asynchronous=False, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='Any', is_required=True, type_object=None, schema_data={'type': 'object'}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x7fd6725b42c0>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x7fd6725b45f0>, method=<bound method BookMeetingRoomPlugin.cancel_room of <__main__.BookMeetingRoomPlugin object at 0x7fd673b90050>>, stream_method=None), 'get_list_of_rooms_available': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='get_list_of_rooms_available', plugin_name='meetingRoomPlugin', description='Get list of booking rooms available', parameters=[], is_prompt=False, is_asynchronous=False, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='Any', is_required=True, type_object=None, schema_data={'type': 'object'}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x7fd6725b46b0>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x7fd6725b4710>, method=<bound method BookMeetingRoomPlugin.get_list_of_rooms_available of <__main__.BookMeetingRoomPlugin object at 0x7fd673b90050>>, stream_method=None)})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register the plugin\n",
    "kernel.remove_all_services()\n",
    "\n",
    "\n",
    "kernel.add_plugin(BookMeetingRoomPlugin(),\"meetingRoomPlugin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The available booking rooms are:\n",
      "\n",
      "1. **Room Azure View**\n",
      "2. **Room Office View**\n",
      "3. **Room Github View**\n"
     ]
    }
   ],
   "source": [
    "# Booking a room\n",
    "\n",
    "#chat_completion = kernel.get_service(type=ChatCompletionClientBase)\n",
    "\n",
    "execution_settings = AzureChatPromptExecutionSettings()\n",
    "execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "chat_history = ChatHistory()\n",
    "\n",
    "chat_history.add_user_message(\"I will like to have the available booking rooms please\")\n",
    "\n",
    "response = (await chat_completion.get_chat_message_contents(\n",
    "      chat_history=chat_history,\n",
    "      settings=execution_settings,\n",
    "      kernel=kernel,\n",
    "      arguments=KernelArguments(),\n",
    "  ))[0]\n",
    "\n",
    "chat_history.add_assistant_message(response.content)\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The room **Room Office View** has been successfully booked for 20 people.\n"
     ]
    }
   ],
   "source": [
    "chat_history.add_user_message(\"Which room is available for 20 people and if possible book it?\")\n",
    "\n",
    "response = (await chat_completion.get_chat_message_contents(\n",
    "      chat_history=chat_history,\n",
    "      settings=execution_settings,\n",
    "      kernel=kernel,\n",
    "      arguments=KernelArguments(),\n",
    "  ))[0]\n",
    "\n",
    "chat_history.add_assistant_message(response.content)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The currently available booking rooms are:\n",
      "\n",
      "1. **Room Azure View**\n",
      "2. **Room Github View**\n"
     ]
    }
   ],
   "source": [
    "chat_history.add_user_message(\"I will like to have the available booking rooms please\")\n",
    "\n",
    "response = (await chat_completion.get_chat_message_contents(\n",
    "      chat_history=chat_history,\n",
    "      settings=execution_settings,\n",
    "      kernel=kernel,\n",
    "      arguments=KernelArguments(),\n",
    "  ))[0]\n",
    "\n",
    "chat_history.add_assistant_message(response.content)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your booking for Room Office View has been successfully canceled.\n"
     ]
    }
   ],
   "source": [
    "chat_history.add_user_message(\"Can you cancel my booking ?\")\n",
    "\n",
    "response = (await chat_completion.get_chat_message_contents(\n",
    "      chat_history=chat_history,\n",
    "      settings=execution_settings,\n",
    "      kernel=kernel,\n",
    "      arguments=KernelArguments(),\n",
    "  ))[0]\n",
    "\n",
    "chat_history.add_assistant_message(response.content)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The currently available booking rooms are:\n",
      "\n",
      "1. Room Azure View  \n",
      "2. Room Office View  \n",
      "3. Room Github View\n"
     ]
    }
   ],
   "source": [
    "chat_history.add_user_message(\"I will like to have the available booking rooms please\")\n",
    "\n",
    "response = (await chat_completion.get_chat_message_contents(\n",
    "      chat_history=chat_history,\n",
    "      settings=execution_settings,\n",
    "      kernel=kernel,\n",
    "      arguments=KernelArguments(),\n",
    "  ))[0]\n",
    "\n",
    "chat_history.add_assistant_message(response.content)\n",
    "\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
