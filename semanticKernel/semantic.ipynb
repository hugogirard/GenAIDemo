{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Semantic Kernel Demo Notebook\n",
    "\n",
    "Welcome to the **Semantic Kernel Demo Notebook**! üéâ\n",
    "\n",
    "In this notebook, we'll explore the powerful capabilities of the Semantic Kernel, a cutting-edge tool designed to enhance your data processing and analysis workflows. Whether you're a data scientist, developer, or just curious about semantic technologies, this demo will provide you with a hands-on experience of how the Semantic Kernel can transform your projects.\n",
    "\n",
    "## What You'll Learn üìö\n",
    "\n",
    "- **Introduction to Semantic Kernel**: Understand the core concepts and architecture.\n",
    "- **Data Processing**: See how the Semantic Kernel can streamline your data workflows.\n",
    "- **Real-World Applications**: Discover practical use cases and examples.\n",
    "\n",
    "## Let's Get Started! üèÅ\n",
    "\n",
    "Ready to dive in? Follow along with the cells below and unleash the power of the Semantic Kernel in your projects. Happy coding! üíª"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install semantic-kernel==1.17.1\n",
    "%pip install python-dotenv==1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the necessary environments variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "#  If we change the .env file you want to override the values loaded in memory\n",
    "load_dotenv(override=True)\n",
    "\n",
    "chat_deployment_name = os.getenv(\"CHAT_DEPLOYMENT_NAME\")\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "base_url = os.getenv(\"OPENAI_BASE_URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is creating the Semantic Kernel object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use the Semantic Kernel following those steps: ü§ñ‚ú®\n",
    "\n",
    "1. **Select the Best AI Service üèÜ**\n",
    "   - Choose the most suitable AI service to run your prompt.\n",
    "\n",
    "2. **Build the Prompt üõ†Ô∏è**\n",
    "   - Construct the prompt using the provided prompt template.\n",
    "\n",
    "3. **Send the Prompt üöÄ**\n",
    "   - Dispatch the prompt to the selected AI service.\n",
    "\n",
    "4. **Receive and Parse the Response üì¨**\n",
    "   - Collect and interpret the response from the AI service.\n",
    "\n",
    "5. **Return the Response üîÑ**\n",
    "   - Deliver the response from the LLM back to your application.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.utils.logging import setup_logging\n",
    "from semantic_kernel.functions import kernel_function\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
    "from semantic_kernel.connectors.ai.chat_completion_client_base import ChatCompletionClientBase\n",
    "from semantic_kernel.contents.chat_history import ChatHistory\n",
    "from semantic_kernel.functions.kernel_arguments import KernelArguments\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai.prompt_execution_settings.azure_chat_prompt_execution_settings import (\n",
    "    AzureChatPromptExecutionSettings,\n",
    ")\n",
    "\n",
    "# Creating the Kernel object\n",
    "\n",
    "kernel =Kernel()\n",
    "\n",
    "chat_completion = AzureChatCompletion(\n",
    "        service_id=\"azure_openai_chat_completion\",\n",
    "        deployment_name=chat_deployment_name,\n",
    "        api_key=api_key,\n",
    "        base_url=base_url\n",
    ")\n",
    "\n",
    "# Adding the chat completion service to the kernel\n",
    "kernel.add_service(chat_completion)\n",
    "\n",
    "# Set the logging level of the kernel to DEBUG\n",
    "setup_logging()\n",
    "logging.basicConfig(\n",
    "    format=\"[%(asctime)s - %(name)s:%(lineno)d - %(levelname)s] %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "logging.getLogger(\"kernel\").setLevel(logging.DEBUG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.services.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.get_service(\"azure_openai_chat_completion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.prompt_template import PromptTemplateConfig, InputVariable\n",
    "\n",
    "prompt = \"\"\"\n",
    "{{$input}} is a dish from where ?\n",
    "\"\"\"\n",
    "\n",
    "execution_config = AzureChatPromptExecutionSettings(\n",
    "    max_tokens=1000,\n",
    "    temperature=0.7,\n",
    "    service_id=\"azure_openai_chat_completion\",\n",
    ")\n",
    "\n",
    "prompt_template_config = PromptTemplateConfig(\n",
    "    template=prompt,\n",
    "    name=\"food\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[\n",
    "       InputVariable(name=\"input\", description=\"The input variable\", is_required=True)\n",
    "    ],\n",
    "    execution_settings=execution_config\n",
    ")\n",
    "\n",
    "food = kernel.add_function(\n",
    "    function_name=\"food\",\n",
    "    plugin_name=\"foodPlugin\",\n",
    "    prompt_template_config=prompt_template_config,\n",
    ")\n",
    "result = await kernel.invoke(food, input=\"Pizza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<coroutine object Kernel.invoke_prompt at 0x7f7de068bab0>\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
