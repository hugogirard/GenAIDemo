{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LangChain 🌐\n",
    "\n",
    "LangChain is a powerful framework designed to simplify the development of applications that leverage language models. It provides a suite of tools and abstractions to help developers build, deploy, and manage language model-based applications efficiently. Whether you're working on chatbots, text generation, or any other NLP task, LangChain has got you covered! 🚀\n",
    "\n",
    "## Key Features ✨\n",
    "\n",
    "- **Modular Design**: Easily integrate with various language models and data sources.\n",
    "- **Scalability**: Built to handle large-scale applications with ease.\n",
    "- **Extensibility**: Customize and extend functionalities to suit your specific needs.\n",
    "- **Community Support**: Active community and extensive documentation to help you get started.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-openai==0.2.14\n",
    "%pip install langchain==0.3.13\n",
    "%pip install python-dotenv==1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load environments variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "chat_deployment_name = os.getenv(\"CHAT_DEPLOYMENT_NAME\")\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "base_url = os.getenv(\"OPENAI_BASE_URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "client = AzureChatOpenAI(\n",
    "    azure_endpoint=base_url,\n",
    "    azure_deployment=chat_deployment_name,\n",
    "    api_key=api_key,\n",
    "    api_version=\"2024-08-01-preview\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the first prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "system_message = SystemMessage(\"\"\"\n",
    "                  You are an assistant and speak french canadian all the time, you cannot speak english.  \n",
    "                  You answer with really strong quebecois accent.\n",
    "                  All your answer always come or are related to the province of Quebec.\"\"\")\n",
    "\n",
    "messages = [\n",
    "    system_message,\n",
    "    HumanMessage(\"Hi where I can find the coolest place to do hiking in Canada?\"),\n",
    "]\n",
    "\n",
    "client.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📜 Prompt Templates\n",
    "\n",
    "Right now, we are passing a list of messages directly into the language model. 🤔 But where does this list of messages come from? Usually, it is constructed from a combination of user input and application logic. 🧩\n",
    "\n",
    "This application logic typically takes the raw user input and transforms it into a list of messages ready to pass to the language model. 🔄 Common transformations include adding a system message or formatting a template with the user input. 📝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_message = SystemMessage(\"\"\"\n",
    "        You are a food assistant, you receive a city and you provide the most popular food in that city based on user {preference}.\n",
    "\"\"\")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        system_message,\n",
    "        (\"user\",\"{city}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build the prompt\n",
    "\n",
    "prompt = prompt_template.invoke({\n",
    "    \"preference\": \"not healhty and really quebecois\",\n",
    "    \"city\": \"Chicoutimi\"\n",
    "})\n",
    "\n",
    "prompt.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the model\n",
    "\n",
    "response = client.invoke(prompt)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Chains in LangChain ⛓️\n",
    "\n",
    "Chains in LangChain are a powerful way to link together multiple components to create complex workflows. By connecting various modules such as language models, tools, and APIs, you can build sophisticated applications that leverage the strengths of each component. Chains allow you to define a sequence of operations, making it easier to manage and maintain your code.\n",
    "\n",
    "With LangChain, you can create both simple and complex chains to handle tasks like data processing, natural language understanding, and more. Whether you're building a chatbot, an automated data analysis tool, or any other application, chains provide a flexible and modular approach to development. 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "template = \"Tell me a joke about {topic}.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful AI bot. Your name is Carl.\"),\n",
    "    (\"human\", template),\n",
    "])\n",
    "\n",
    "# Use a string parser and to avoid need to do result.content\n",
    "#  So this add the prompt --> chat --> end you parse the output\n",
    "chain = prompt_template | client | StrOutputParser()\n",
    "\n",
    "result = await chain.ainvoke({\"topic\": \"cat\"})\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a comedian who tells jokes about {topic}\"),\n",
    "    (\"human\", \"Tell me {joke_count} jokes.\"),\n",
    "])\n",
    "\n",
    "uppercase_output = RunnableLambda(lambda x: x.upper())\n",
    "count_words = RunnableLambda(lambda x: f\"Words count: {len(x.split())}\\n{x}\")\n",
    "\n",
    "chain = prompt_template | client | StrOutputParser() | uppercase_output | count_words\n",
    "\n",
    "result = await chain.ainvoke({\"topic\": \"cat\", \"joke_count\": 3})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda, RunnableParallel\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are an expert product reviewer\"),\n",
    "    (\"human\", \"List the main features of the product {product_name}.\")\n",
    "])\n",
    "\n",
    "def analyze_pros(features):\n",
    "    pros_template = ChatPromptTemplate([\n",
    "        (\"system\", \"You are an expert product reviewer\"),\n",
    "        (\"human\", \"Given these features: {features}, list the pros of these features.\")\n",
    "    ])\n",
    "    return pros_template.format_prompt(features=features)\n",
    "\n",
    "def analyze_cons(features):\n",
    "    cons_template = ChatPromptTemplate([\n",
    "        (\"system\", \"You are an expert product reviewer\"),\n",
    "        (\"human\", \"Given these features: {features}, list the cons of these features.\")\n",
    "    ])\n",
    "    return cons_template.format_prompt(features=features)\n",
    "\n",
    "def combine_pros_cons(pros, cons):\n",
    "    return f\"Pros:\\n {pros}\\n\\nCons:\\n {cons}\"\n",
    "\n",
    "pros_branch = (\n",
    "    RunnableLambda(lambda x: analyze_pros(x)) | client | StrOutputParser()\n",
    ")\n",
    "\n",
    "cons_branch = (\n",
    "    RunnableLambda(lambda x: analyze_cons(x)) | client | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    prompt_template\n",
    "    | client\n",
    "    | StrOutputParser()\n",
    "    | RunnableParallel(branches={\"pros\": pros_branch, \"cons\": cons_branch})\n",
    "    | RunnableLambda(lambda x: combine_pros_cons(x[\"branches\"][\"pros\"], x[\"branches\"][\"cons\"])) \n",
    ")\n",
    "\n",
    "result = await chain.ainvoke({\"product_name\": \"iPhone 13\"})\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_weather(region: str) -> str:\n",
    "    \"\"\"Get the regional weather for Middle-Earth from the region passed as a parameter\"\"\"\n",
    "    weather_data = {\n",
    "        \"Shire\": \"Sunny\",\n",
    "        \"Mordor\": \"Hot and dry\",\n",
    "        \"Rivendell\": \"Mild and rainy\",\n",
    "        \"Gondor\": \"Warm and breezy\",\n",
    "        \"Rohan\": \"Windy\",\n",
    "        \"Mirkwood\": \"Foggy and damp\",\n",
    "        \"Isengard\": \"Stormy\",\n",
    "        \"Lothlorien\": \"Pleasant and cool\"\n",
    "    }\n",
    "    \n",
    "   # Normalize the region name to lower case for comparison\n",
    "    region = region.lower()\n",
    "    \n",
    "    for key in weather_data:\n",
    "        if key.lower() in region:\n",
    "            return weather_data[key]\n",
    "    \n",
    "    return \"Unknown region\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_i1AxxC4ajVjSjEtnguRxrWtC', 'function': {'arguments': '{\"region\":\"Mordor\"}', 'name': 'get_weather'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 61, 'total_tokens': 77, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-11-20', 'system_fingerprint': 'fp_82ce25c0d4', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'tool_calls', 'logprobs': None, 'content_filter_results': {}}, id='run-d01c9e27-4665-43ac-9077-742d0381d93d-0', tool_calls=[{'name': 'get_weather', 'args': {'region': 'Mordor'}, 'id': 'call_i1AxxC4ajVjSjEtnguRxrWtC', 'type': 'tool_call'}], usage_metadata={'input_tokens': 61, 'output_tokens': 16, 'total_tokens': 77, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [get_weather]\n",
    "\n",
    "client_with_tools = client.bind_tools(tools)\n",
    "\n",
    "client_with_tools.invoke(\"What is the weather in the Mordor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'get_weather',\n",
       "  'args': {'region': 'Shire'},\n",
       "  'id': 'call_WqJ2nO50T4P07RdQndEbTVPv',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_with_tools.invoke(\"What is the weather in the Shire\").tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ToolMessage(content='Hot and dry', name='get_weather', tool_call_id='call_oDPh7z1fmaAbio5uYEaKeLGr')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [get_weather]\n",
    "\n",
    "client_with_tools = client.bind_tools(tools)\n",
    "\n",
    "ai_tools = client_with_tools.invoke(\"What is the weather in the Mordor\")\n",
    "\n",
    "messages = []\n",
    "\n",
    "for tool_call in ai_tools.tool_calls:\n",
    "    selected_tool = {\"get_weather\": get_weather}[tool_call[\"name\"].lower()]\n",
    "    tool_msg = selected_tool.invoke(tool_call)\n",
    "    messages.append(tool_msg)\n",
    "\n",
    "messages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
